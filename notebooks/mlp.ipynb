{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002025127410888672\n",
      "Execution time: 6.956 ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from neural_poisson.model.encoder import MLP\n",
    "import time\n",
    "\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "x = torch.randn((10_000, 3), device=\"cuda\")\n",
    "mlp = MLP(in_dim=3, out_dim=1, hidden_dim=256, num_layers=8).to(\"cuda\") \n",
    "ts = []\n",
    "for _ in range(1):\n",
    "    # torch.cuda.synchronize()\n",
    "    s = time.time()\n",
    "    start_event.record()\n",
    "    mlp(x)\n",
    "    mlp(x)\n",
    "    mlp(x)\n",
    "    # torch.cuda.synchronize()\n",
    "    e = time.time() - s\n",
    "    end_event.record()\n",
    "    ts.append(e)\n",
    "print(sum(ts))\n",
    "time.sleep(1)\n",
    "elapsed_time = start_event.elapsed_time(end_event)  # Gives time in ms\n",
    "print(f\"Execution time: {elapsed_time:.3f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6201214790344238\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((10_000, 3), device=\"cuda\")\n",
    "mlp = MLP(in_dim=3, out_dim=1, hidden_dim=256, num_layers=8).to(\"cuda\") \n",
    "ts = []\n",
    "for _ in range(100):\n",
    "    torch.cuda.synchronize()\n",
    "    s = time.time()\n",
    "    xs = torch.stack([x, x, x])\n",
    "    mlp(xs)\n",
    "torch.cuda.synchronize()\n",
    "    e = time.time() - s\n",
    "    ts.append(e)\n",
    "print(sum(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0010,  0.0030, -0.0015],\n",
       "        [ 0.0024,  0.0007, -0.0003],\n",
       "        [ 0.0011,  0.0018, -0.0024],\n",
       "        ...,\n",
       "        [ 0.0023,  0.0013, -0.0015],\n",
       "        [ 0.0008,  0.0018,  0.0003],\n",
       "        [ 0.0001,  0.0016, -0.0007]], device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from neural_poisson.model.encoder import MLP\n",
    "\n",
    "device = \"cuda\"\n",
    "N = 1_000\n",
    "points = torch.rand((N, 3), requires_grad=True).to(\"cuda\")\n",
    "mlp = MLP().to(\"cuda\")\n",
    "x = mlp(points)\n",
    "\n",
    "_x = 0.5 * (x**2).sum()\n",
    "# _x = x.sum()\n",
    "\n",
    "# we want to compute dX/dp, which is the gradient of the estimated indicator function \n",
    "# X w.r.t the input points. However we can only compute dL/dp which computes the \n",
    "# gradients from a loss scalar. The chain rule is dL/dp = dL/dX * dX/dp. In order to \n",
    "# compute dX/dp we need to define the loss function do get dL/dX = 1, which results in\n",
    "# a simple summation of dX, e.g. L=X.sum(), where the derivatives are 1.\n",
    "dX = torch.autograd.grad(x.sum(), points)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0027997493743896484\n"
     ]
    }
   ],
   "source": [
    "x = time.time()\n",
    "p = mlp(points)\n",
    "x = time.time() - x\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9457, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.vector_norm(points, dim=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.3827e-07, -6.2139e-06,  2.7392e-06],\n",
       "        [ 1.6456e-06, -6.4065e-06,  1.2606e-06],\n",
       "        [ 3.8234e-07, -2.8763e-06,  2.7467e-06],\n",
       "        ...,\n",
       "        [-2.3553e-06, -5.2600e-07,  3.0943e-07],\n",
       "        [-1.6997e-06, -1.5128e-06,  3.3624e-06],\n",
       "        [ 5.2575e-07, -4.5885e-06,  2.0370e-06]], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from neural_poisson.model.activation import activation_fn\n",
    "\n",
    "class MLP(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int = 3,\n",
    "        out_dim: int = 1,\n",
    "        hidden_dim: int = 256,\n",
    "        num_layers: int = 5,\n",
    "        activation: str = \"relu\",\n",
    "    ):\n",
    "        layers: list[Any] = []\n",
    "\n",
    "        # input layers\n",
    "        layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "        layers.append(activation_fn(activation))\n",
    "\n",
    "        # hidden layers\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(activation_fn(activation))\n",
    "\n",
    "        # output layer\n",
    "        layers.append(nn.Linear(hidden_dim, out_dim))\n",
    "\n",
    "        # self.mlp = nn.Sequential(*layers)\n",
    "        super().__init__(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super().forward(x)\n",
    "\n",
    "import torch\n",
    "mlp = MLP()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2dgs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
